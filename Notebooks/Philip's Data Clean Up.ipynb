{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dependencies\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from uszipcode import SearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data files to import\n",
    "url2017='https://hfdapp.houstontx.gov/311/311-Public-Data-Extract-2017.txt'\n",
    "url2018='https://hfdapp.houstontx.gov/311/311-Public-Data-Extract-2018.txt'\n",
    "url2019='https://hfdapp.houstontx.gov/311/311-Public-Data-Extract-2019.txt'\n",
    "url2020='https://hfdapp.houstontx.gov/311/311-Public-Data-Extract-monthly.txt'\n",
    "nullzip=pd.read_csv('../Clean Data Files/311latlngzipcodes.csv',dtype={'calczip':str})\n",
    "\n",
    "#Define dataframe column names and select numeric and date columns\n",
    "cols=['case number','sr location','county','district','neighborhood','tax id','trash quad','recycle quad','trash day','heavy trash day','recycle day','key map',\n",
    "      'management district','department','division','sr type','queue','sla','status','sr create date','due date','date closed','overdue','title','x','y','latitude',\n",
    "      'longitude','channel type']\n",
    "numcols=['latitude','longitude']\n",
    "datecols=['sr create date','due date','date closed']\n",
    "\n",
    "#Create zipcode retrieval function\n",
    "search=SearchEngine(simple_zipcode=False)\n",
    "def zipinfo(lat,lng):\n",
    "    zipdata=search.by_coordinates(lat,lng,radius=3,returns=1)\n",
    "    for zipcode in zipdata:\n",
    "        return zipcode.zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 9979: expected 29 fields, saw 30\\nSkipping line 16339: expected 29 fields, saw 30\\n'\n",
      "b'Skipping line 211068: expected 29 fields, saw 30\\n'\n",
      "b'Skipping line 294299: expected 29 fields, saw 30\\n'\n",
      "b'Skipping line 327924: expected 29 fields, saw 30\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364664, 30)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create 2017 data frames\n",
    "data2017=pd.read_csv(url2017,header=5,sep='|',error_bad_lines=False)\n",
    "data2017=data2017.drop(data2017.index[0]).reset_index(drop=True)\n",
    "data2017.columns=cols\n",
    "data2017[cols]=data2017[cols].apply(lambda x:x.str.strip()).replace(r'^\\s*$',np.nan,regex=True)\n",
    "data2017[datecols]=data2017[datecols].apply(pd.to_datetime,format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "data2017[numcols]=data2017[numcols].apply(pd.to_numeric,errors='coerce')\n",
    "top2017=data2017['sr type'].value_counts()[lambda x:x>=10000].index.tolist()\n",
    "data2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 124864: expected 29 fields, saw 30\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(399953, 30)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create 2018 data frames\n",
    "data2018=pd.read_csv(url2018,header=5,sep='|',error_bad_lines=False)\n",
    "data2018=data2018.drop(data2018.index[0]).reset_index(drop=True)\n",
    "data2018.columns=cols\n",
    "data2018[cols]=data2018[cols].apply(lambda x:x.str.strip()).replace(r'^\\s*$',np.nan,regex=True)\n",
    "data2018[datecols]=data2018[datecols].apply(pd.to_datetime,format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "data2018[numcols]=data2018[numcols].apply(pd.to_numeric,errors='coerce')\n",
    "top2018=data2018['sr type'].value_counts()[lambda x:x>=10000].index.tolist()\n",
    "data2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 86859: expected 29 fields, saw 31\\n'\n",
      "b'Skipping line 124913: expected 29 fields, saw 30\\n'\n",
      "b'Skipping line 144497: expected 29 fields, saw 30\\n'\n",
      "b'Skipping line 218652: expected 29 fields, saw 31\\n'\n",
      "b'Skipping line 349873: expected 29 fields, saw 30\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(395258, 30)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create 2019 data frames\n",
    "data2019=pd.read_csv(url2019,header=5,sep='|',error_bad_lines=False)\n",
    "data2019=data2019.drop(data2019.index[0]).reset_index(drop=True)\n",
    "data2019.columns=cols\n",
    "data2019[cols]=data2019[cols].apply(lambda x:x.str.strip()).replace(r'^\\s*$',np.nan,regex=True)\n",
    "data2019[datecols]=data2019[datecols].apply(pd.to_datetime,format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "data2019[numcols]=data2019[numcols].apply(pd.to_numeric,errors='coerce')\n",
    "top2019=data2019['sr type'].value_counts()[lambda x:x>=10000].index.tolist()\n",
    "data2019.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1058859, 39)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create all complete years dataframe\n",
    "tempdata311=data2017.append([data2018,data2019])\n",
    "data311=pd.merge(tempdata311,nullzip,on=['latitude','longitude'],how='left')\n",
    "data311['year']=data311['sr create date'].dt.strftime('%Y')\n",
    "data311['month']=data311['sr create date'].dt.strftime('%m')\n",
    "data311=data311[pd.notnull(data311['latitude'])]\n",
    "data311['truezip']='77'+data311['sr location'].str.extract(r'77(\\d{3}\\-?\\d{0,4})')\n",
    "data311['zipcode']=np.where(data311['truezip'].isnull()==True,data311['calczip'],data311['truezip'])\n",
    "data311['openclosetime']=data311['date closed']-data311['sr create date']\n",
    "data311['daystoclose']=data311['openclosetime']/timedelta(days=1)\n",
    "data311['openduetime']=data311['due date']-data311['sr create date']\n",
    "data311['daysdue']=data311['openduetime']/timedelta(days=1)\n",
    "data311['missedduedate']=np.where(data311['due date']>data311['date closed'],0,1)\n",
    "types311=data311.groupby(['sr type','year'])['case number'].count().unstack('year').reset_index()\n",
    "types311.columns=['sr type','2017','2018','2019']\n",
    "data311.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2020 data frames\n",
    "data2020=pd.read_csv(url2020,header=5,sep='|',error_bad_lines=False)\n",
    "data2020=data2020.drop(data2020.index[0]).reset_index(drop=True)\n",
    "data2020.columns=cols\n",
    "data2020[cols]=data2020[cols].apply(lambda x:x.str.strip()).replace(r'^\\s*$',np.nan,regex=True)\n",
    "data2020[datecols]=data2020[datecols].apply(pd.to_datetime,errors='coerce')\n",
    "data2020[numcols]=data2020[numcols].apply(pd.to_numeric,errors='coerce')\n",
    "data2020=data2020[pd.notnull(data2020['latitude'])]\n",
    "data2020['zipcode']='77'+data2020['sr location'].str.extract(r'77(\\d{3}\\-?\\d{0,4})')\n",
    "data2020['openclosetime']=data2020['date closed']-data2020['sr create date']\n",
    "data2020['openduetime']=data2020['due date']-data2020['sr create date']\n",
    "top2020=data2020['sr type'].value_counts()[lambda x:x>=10000].index.tolist()\n",
    "data2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create csv of number of service requests by type per year\n",
    "types311.to_csv('../Clean Data Files/Houston 311 SR Types by Year.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe containing service requests with around 10000 a year\n",
    "toprequests=sorted(np.unique(top2017+top2018+top2019+top2020))\n",
    "topdata=data311.loc[data311['sr type'].isin(toprequests)].reset_index()\n",
    "topdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display null values in dataset\n",
    "topdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingzip=topdata[topdata.zipcode.isnull()]\n",
    "missingzip=missingzip[['latitude','longitude']].reset_index()\n",
    "missingzip.drop_duplicates(keep=False,inplace=True)\n",
    "#zipmiss=missingzip[0:5000]\n",
    "missingzip['calczip']=np.vectorize(zipinfo)(missingzip['latitude'].values,missingzip['longitude'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newnullzip=nullzip.append([missingzip],sort=False)\n",
    "newnullzip.drop(['index'],axis=1,inplace=True)\n",
    "newnullzip.drop_duplicates(keep=False,inplace=True)\n",
    "newnullzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newnullzip.to_csv('../Clean Data Files/311latlngzipcodes.csv',index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
